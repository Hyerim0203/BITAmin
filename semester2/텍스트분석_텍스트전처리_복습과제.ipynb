{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 문제1_데이터 전처리\n",
    "insta.txt 데이터는 corona를 태그로한 약 instagram 게시글을 크롤링한 데이터 입니다. 해당 데이터를 이용하여 인스타그램 태그를 이용한 분석을 하고 싶습니다. 이를 위한 데이터 전처리 함수를 만들고 전처리를 진행하여 주세요.\n",
    "1. 데이터를 불러온 후, 하나의 문자열로 읽어오세요  \n",
    "2. 빈칸을 채워주세요\n",
    "    - 빈칸1 : 해당 텍스트 안에는 게시글이 POST+숫자(ex: POST94)로 구분되어 있습니다. 이를 기준으로 각 게시글을 분리할 수 있도록 빈칸에 정규표현식을 작성하세요\n",
    "    - 빈칸2 : 추출한 게시글에서 태그만을 추출할 수 있도록 빈칸에 정규표현식을 작성하세요\n",
    "        - #을 포함하여 추출하세요\n",
    "        - #뒤의 영숫자 개수가 2개 이상인 것만 추출할 수 있게 작성하세요\n",
    "        - ex) \"\\s\"\n",
    "    - 빈칸3: RegexpTokenizer클래스의 tokenize 함수를 이용하여 문장을 구분하세요\n",
    "        - tokenize 함수 : 텍스트를 인자로 받아서 해당 분류기를 이용하여 텍스트를 분류, 이를 각 리스트 원소로 한 리스트 반환\n",
    "        - ex) RegxpTokenizer(정규표현식).tokenize(text)\n",
    "    - 빈칸4: RegexpTokenizer클래스의 tokenize를 이용하여 각 문장에서 단어를 추출하세요.\n",
    "    - 빈칸5 : words 리스트 안의 각 단어에서 \"#\"을 제거한 후 모두 소문자로 변경하고 개별 단어가 스톱워드의 단어에 포함되지 않으면 filtered_words 리스트에 추가하세요.\n",
    "    - 빈칸6 : 해당 리스트 안에서의 중복된 단어를 제거하세요.  \n",
    "1. 함수를 실행하세요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\이혜림\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import nltk\n",
    "nltk.download(\"stopwords\") # 언어별 스톱 워드를 제공\n",
    "stopwords = nltk.corpus.stopwords.words(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = open(\"./프로젝트/insta.txt\").read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 인스타그램 게시글에서 태그만을 추출하는 함수\n",
    "def tags_extraction(data):\n",
    "    import re \n",
    "    from nltk.tokenize import RegexpTokenizer # 정규표현식을 사용할 수 있는 토큰화함수\n",
    "    \n",
    "    words = [] # 단어 토큰을 저장할 list\n",
    "    filtered_words = [] # 필터화된 단어 토큰을 저장할 list\n",
    "    \n",
    "    \"\"\"\n",
    "    RegexpTokenizer(정규표현식, gaps=False)\n",
    "    gaps=True이면 해당 정규표현식을 기준으로 분류\n",
    "    gaps=False이면 해당 정규표현식만을 추출\n",
    "    \"\"\"\n",
    "    tokenizer_sentence = RegexpTokenizer(\"POST\\d\", gaps=True) # 문장 토큰 분류기\n",
    "    tokenizer_word=RegexpTokenizer(\"#\\w\\w+\") # 단어 토큰 분류기\n",
    "    \n",
    "    sentences = tokenizer_sentence.tokenize(data)\n",
    "        \n",
    "    for sentence in sentences:\n",
    "        words += tokenizer_word.tokenize(sentence)\n",
    "   \n",
    "    # words 리스트 안의 단어를 \"#\"을 제거한 후 모두 소문자로 변경하고 개별 단어가 스톱 워드의 단어에 포함되지 않으면 filter_words에 추가\n",
    "    for idx, w in enumerate(words):\n",
    "        w = w[1:].lower() # # 제거 및 소문자로 변경\n",
    "        if w not in stopwords:\n",
    "            filtered_words.append(w)\n",
    "    \n",
    "    # words 리스트 안의 단어의 중복을 제거하세요\n",
    "    return list(set(filtered_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags_extraction(data) # 실행하세요"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 문제3_CountVectorizer\n",
    "- disney_plus_shows.csv 데이터는 디즈니+스트림 서비스에서 이용할 수 있는 쇼에 대한 정보입니다. 데이터를 사용하여 밑의 문제를 풀어주세요 !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 데이터의 모든 결측치를 제거하여 주세요.(행 기준)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 데이터 불러오기\n",
    "d_show = pd.read_csv('disney_plus_shows.csv', index_col=0)[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "title          0\n",
      "plot           0\n",
      "type           0\n",
      "rated          0\n",
      "year           0\n",
      "released_at    0\n",
      "added_at       0\n",
      "runtime        0\n",
      "genre          0\n",
      "director       0\n",
      "writer         0\n",
      "actors         0\n",
      "language       0\n",
      "country        0\n",
      "awards         0\n",
      "metascore      0\n",
      "imdb_rating    0\n",
      "imdb_votes     0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# 결측치 제거\n",
    "d_show.dropna(how='any', inplace=True)\n",
    "print(d_show.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- CountVectorizer() 메소드를 이용하여 count_vec 객체를 만들어주세요.\n",
    "- 파리미터 옵션\n",
    "    + stop_words=\"english\"\n",
    "    + analyzer='word', \n",
    "    + ngram_range=(1, 2)\n",
    "    + min_df=1\n",
    "    + max_features=None\n",
    "- 생성된 객체에 d_show데이터의 plot 변수를 넣어주세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                ngram_range=(1, 2), preprocessor=None, stop_words='english',\n",
       "                strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 객체 생성\n",
    "count_vec = CountVectorizer(stop_words=\"english\", analyzer='word', \n",
    "                            ngram_range=(1, 2),  min_df=1, max_features=None)\n",
    "# plot 변수 fitting\n",
    "count_vec.fit(d_show['plot'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- get_feature_names()를 사용해 데이터의 단어들을 5칸 간격 차이로 출력해 주세요.\n",
    "- vocabulary_를 사용해 단어들이 각각 몇번씩 쓰였는지 출력해주세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<Every 5rd feature>\n",
      "['coat', 'coat instead', 'creates', 'creates extravagant', 'cruella', 'cruella devil', 'dalmatian', 'dalmatian puppies', 'date', 'date ill', 'designer', 'designer plots', 'devil', 'devil gets', 'does', 'evil', 'evil high', 'extravagant', 'extravagant fur', 'extravagant mess', 'fashion', 'fashion designer', 'fur', 'fur coat', 'gets', 'gets prison', 'goes', 'goes puppies', 'high', 'high fashion', 'ill', 'ill tempered', 'instead', 'instead creates', 'make', 'make extravagant', 'mess', 'older', 'older sister', 'order', 'order make', 'plots', 'plots steal', 'popular', 'popular teenager', 'pretty', 'pretty popular', 'prison', 'prison goes', 'puppies', 'puppies order', 'sister', 'sister does', 'steal', 'steal dalmatian', 'teenager', 'teenager date', 'tempered', 'tempered older']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'pretty': 45,\n",
       " 'popular': 43,\n",
       " 'teenager': 55,\n",
       " 'date': 8,\n",
       " 'ill': 30,\n",
       " 'tempered': 57,\n",
       " 'older': 37,\n",
       " 'sister': 51,\n",
       " 'does': 14,\n",
       " 'pretty popular': 46,\n",
       " 'popular teenager': 44,\n",
       " 'teenager date': 56,\n",
       " 'date ill': 9,\n",
       " 'ill tempered': 31,\n",
       " 'tempered older': 58,\n",
       " 'older sister': 38,\n",
       " 'sister does': 52,\n",
       " 'evil': 15,\n",
       " 'high': 28,\n",
       " 'fashion': 20,\n",
       " 'designer': 10,\n",
       " 'plots': 41,\n",
       " 'steal': 53,\n",
       " 'dalmatian': 6,\n",
       " 'puppies': 49,\n",
       " 'order': 39,\n",
       " 'make': 34,\n",
       " 'extravagant': 17,\n",
       " 'fur': 22,\n",
       " 'coat': 0,\n",
       " 'instead': 32,\n",
       " 'creates': 2,\n",
       " 'mess': 36,\n",
       " 'evil high': 16,\n",
       " 'high fashion': 29,\n",
       " 'fashion designer': 21,\n",
       " 'designer plots': 11,\n",
       " 'plots steal': 42,\n",
       " 'steal dalmatian': 54,\n",
       " 'dalmatian puppies': 7,\n",
       " 'puppies order': 50,\n",
       " 'order make': 40,\n",
       " 'make extravagant': 35,\n",
       " 'extravagant fur': 18,\n",
       " 'fur coat': 23,\n",
       " 'coat instead': 1,\n",
       " 'instead creates': 33,\n",
       " 'creates extravagant': 3,\n",
       " 'extravagant mess': 19,\n",
       " 'cruella': 4,\n",
       " 'devil': 12,\n",
       " 'gets': 24,\n",
       " 'prison': 47,\n",
       " 'goes': 26,\n",
       " 'cruella devil': 5,\n",
       " 'devil gets': 13,\n",
       " 'gets prison': 25,\n",
       " 'prison goes': 48,\n",
       " 'goes puppies': 27}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get_feature_names() 사용\n",
    "print(\"\\n<Every 5rd feature>\\n{}\".format(count_vec.get_feature_names()))\n",
    "# vocabulary_ 사용\n",
    "count_vec.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 문제4 COO, CSR 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_array = np.array([[0,2,3,0,1,4],\n",
    "                  [3,5,9,0,0,8],\n",
    "                  [4,3,0,1,2,8],\n",
    "                  [2,0,9,9,1,0],\n",
    "                  [0,0,2,0,9,1],\n",
    "                  [8,2,0,3,0,0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 0이 아닌 데이터 추출\n",
    "- 행 위치와 열 위치를 각각 array로 생성\n",
    "- 행 위치 배열의 고유한 값의 시작 위치 인덱스를 배열로 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0이 아닌 데이터 추출\n",
    "data = np.array([2,3,1,4,3,5,9,8,4,3,1,2,8,2,9,9,1,2,9,2,8,2,3])\n",
    "\n",
    "# 행 위치\n",
    "row_pos = np.array([0,0,0,0,1,1,1,1,2,2,2,2,2,3,3,3,3,4,4,4,5,5,5])\n",
    "# 열 위치\n",
    "col_pos = np.array([1,2,4,5,0,1,2,5,0,1,3,4,5,0,2,3,4,2,4,5,0,1,3])\n",
    "# 고유한 인덱스 배열\n",
    "row_pos_ind = np.array([0,4,8,13,17,20,23])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- COO 형식으로 변환\n",
    "- CSR 형식으로 변환\n",
    "- toarray()를 통해 출력 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 2 3 0 1 4]\n",
      " [3 5 9 0 0 8]\n",
      " [4 3 0 1 2 8]\n",
      " [2 0 9 9 1 0]\n",
      " [0 0 2 0 9 2]\n",
      " [8 2 0 3 0 0]]\n",
      "[[0 2 3 0 1 4]\n",
      " [3 5 9 0 0 8]\n",
      " [4 3 0 1 2 8]\n",
      " [2 0 9 9 1 0]\n",
      " [0 0 2 0 9 2]\n",
      " [8 2 0 3 0 0]]\n"
     ]
    }
   ],
   "source": [
    "# COO 형식으로 변환\n",
    "sparse_coo = sparse.coo_matrix((data, (row_pos,col_pos)))\n",
    "# CSR 형식으로 변환\n",
    "sparse_csr = sparse.csr_matrix((data,col_pos,row_pos_ind))\n",
    "print(sparse_coo.toarray())\n",
    "print()\n",
    "print(sparse_csr.toarray())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
