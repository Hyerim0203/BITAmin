{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 문제1_데이터 전처리\n",
    "insta.txt 데이터는 corona를 태그로한 약 instagram 게시글을 크롤링한 데이터 입니다. 해당 데이터를 이용하여 인스타그램 태그를 이용한 분석을 하고 싶습니다. 이를 위한 데이터 전처리 함수를 만들고 전처리를 진행하여 주세요.\n",
    "1. 데이터를 불러온 후, 하나의 문자열로 읽어오세요  \n",
    "2. 빈칸을 채워주세요\n",
    "    - 빈칸1 : 해당 텍스트 안에는 게시글이 POST+숫자(ex: POST94)로 구분되어 있습니다. 이를 기준으로 각 게시글을 분리할 수 있도록 빈칸에 정규표현식을 작성하세요\n",
    "    - 빈칸2 : 추출한 게시글에서 태그만을 추출할 수 있도록 빈칸에 정규표현식을 작성하세요\n",
    "        - #을 포함하여 추출하세요\n",
    "        - #뒤의 영숫자 개수가 2개 이상인 것만 추출할 수 있게 작성하세요\n",
    "        - ex) \"\\s\"\n",
    "    - 빈칸3: RegexpTokenizer클래스의 tokenize 함수를 이용하여 문장을 구분하세요\n",
    "        - tokenize 함수 : 텍스트를 인자로 받아서 해당 분류기를 이용하여 텍스트를 분류, 이를 각 리스트 원소로 한 리스트 반환\n",
    "        - ex) RegxpTokenizer(정규표현식).tokenize(text)\n",
    "    - 빈칸4: RegexpTokenizer클래스의 tokenize를 이용하여 각 문장에서 단어를 추출하세요.\n",
    "    - 빈칸5 : words 리스트 안의 각 단어에서 \"#\"을 제거한 후 모두 소문자로 변경하고 개별 단어가 스톱워드의 단어에 포함되지 않으면 filtered_words 리스트에 추가하세요.\n",
    "    - 빈칸6 : 해당 리스트 안에서의 중복된 단어를 제거하세요.  \n",
    "1. 함수를 실행하세요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\이혜림\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import nltk\n",
    "nltk.download(\"stopwords\") # 언어별 스톱 워드를 제공\n",
    "stopwords = nltk.corpus.stopwords.words(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = open(\"./프로젝트/insta.txt\").read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 인스타그램 게시글에서 태그만을 추출하는 함수\n",
    "def tags_extraction(data):\n",
    "    import re \n",
    "    from nltk.tokenize import RegexpTokenizer # 정규표현식을 사용할 수 있는 토큰화함수\n",
    "    \n",
    "    words = [] # 단어 토큰을 저장할 list\n",
    "    filtered_words = [] # 필터화된 단어 토큰을 저장할 list\n",
    "    \n",
    "    \"\"\"\n",
    "    RegexpTokenizer(정규표현식, gaps=False)\n",
    "    gaps=True이면 해당 정규표현식을 기준으로 분류\n",
    "    gaps=False이면 해당 정규표현식만을 추출\n",
    "    \"\"\"\n",
    "    tokenizer_sentence = RegexpTokenizer(\"POST\\d\", gaps=True) # 문장 토큰 분류기\n",
    "    tokenizer_word=RegexpTokenizer(\"#\\w\\w+\") # 단어 토큰 분류기\n",
    "    \n",
    "    sentences = tokenizer_sentence.tokenize(data)\n",
    "        \n",
    "    for sentence in sentences:\n",
    "        words += tokenizer_word.tokenize(sentence)\n",
    "   \n",
    "    # words 리스트 안의 단어를 \"#\"을 제거한 후 모두 소문자로 변경하고 개별 단어가 스톱 워드의 단어에 포함되지 않으면 filter_words에 추가\n",
    "    for idx, w in enumerate(words):\n",
    "        w = w[1:].lower() # # 제거 및 소문자로 변경\n",
    "        if w not in stopwords:\n",
    "            filtered_words.append(w)\n",
    "    \n",
    "    # words 리스트 안의 단어의 중복을 제거하세요\n",
    "    return list(set(filtered_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags_extraction(data) # 실행하세요"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
