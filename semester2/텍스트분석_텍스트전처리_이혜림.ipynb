{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 텍스트분석"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 머신러닝 알고리즘은 숫자형의 피처기반 데이터만 입력받을 수 있음\n",
    "- 픽터 벡터화 : 텍스트를 word 기반의 다수의 피처로 추출하고 이 피처에 단어 빈도수와 같은 숫자 값을 부여하여 단어의 조합인 벡터값으로 표현하는 것\n",
    "    - BOW(Bag of Words)\n",
    "    - Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 텍스트 분석 수행 프로세스\n",
    "\n",
    "1. 텍스트 사전 준비작업 : 클렌징, 대/소문자 변경, 특수문자 삭제 등의 클렌징 작업, 토큰화 작업, 의미 없는 단어 제거(Stop word) 작업, 어근 추출 등의 텍스트 정규화 작업\n",
    "1. 피터 벡터화/추출 : 가공된 텍스트에서 피처를 추출하고 여기에 벡터 값을 할당, BOW, Word2Vec\n",
    "1. ML 모델 수립 및 학습/예측/평가"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 텍스트 사전 준비작업"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 클렌징\n",
    "- 클렌징 : 텍스트에서 분석에 오히려 방해가 되는 불필요한 문자, 기호 등을 사전에 제거하는 작업. ex) HTML, XML 태그/특정 기호"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 텍스트 토큰화\n",
    "- 문서에서 문장을 분리하는 **문장 토큰화**와 문장에서 단어를 분리하는 **단어 토큰화**\n",
    "    - 문장 토큰화 : 마침표, 개행문자(\\n) 등 문장의 마지막을 뜻하는 기호에 따라 분리.\n",
    "    - 단어 토큰화 : 문장을 단어로 토큰화. 공백, 콤마, 마침표, 개행문자 등으로 단어를 분리. 단어의 순서가 중요하지 않은 경우 문장 토큰화 없이 단어 토큰화만 사용해도 충분.(단어 토큰화하면서 각 문장이 다 떼어지기 때문)\n",
    "        - 단어 토큰화를 하게 되면 문맥적인 의미는 무시될 수 밖에 없음. 이러한 문제를 해결하기 위해 도입된 것이 n-gram,\n",
    "        - n-gram은 연속된 n개의 단어를 하나의 토큰화 단위로 분리해 냄.\n",
    "        - ex) \"Agent smith knokds the door\" -> 2-gram\n",
    "            - (Agent, smith), (smith, knokds), (knokds, the), (the, door)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 문장 토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import sent_tokenize # 문장 토큰화 import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\이혜림\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download(\"punkt\") # 단어사전과 같이 참조가 필요한 데이터 세트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_sample = \"The Matrix is everywhere its all aroud us, here even in the room.\\\n",
    "    You can see it out your window or on your television.\\\n",
    "    You feel it when you go to work, or go to church or pay your taxes.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'> 3\n",
      "['The Matrix is everywhere its all aroud us, here even in the room.', 'You can see it out your window or on your television.', 'You feel it when you go to work, or go to church or pay your taxes.']\n"
     ]
    }
   ],
   "source": [
    "sentences = sent_tokenize(text=text_sample)\n",
    "print(type(sentences), len(sentences))\n",
    "print(sentences)\n",
    "# 각각의 문장으로 구성된 list 객체를 반환"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 단어 토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The',\n",
       " 'Matrix',\n",
       " 'is',\n",
       " 'everywhere',\n",
       " 'its',\n",
       " 'all',\n",
       " 'aroud',\n",
       " 'us',\n",
       " ',',\n",
       " 'here',\n",
       " 'even',\n",
       " 'in',\n",
       " 'the',\n",
       " 'room',\n",
       " '.']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import word_tokenize # 단어 토큰화 import\n",
    "word_tokenize(sentences[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 문장 토큰화 & 단어 토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'> 3\n",
      "[['The', 'Matrix', 'is', 'everywhere', 'its', 'all', 'aroud', 'us', ',', 'here', 'even', 'in', 'the', 'room', '.'], ['You', 'can', 'see', 'it', 'out', 'your', 'window', 'or', 'on', 'your', 'television', '.'], ['You', 'feel', 'it', 'when', 'you', 'go', 'to', 'work', ',', 'or', 'go', 'to', 'church', 'or', 'pay', 'your', 'taxes', '.']]\n"
     ]
    }
   ],
   "source": [
    "# 여러 개의 문장으로 된 입력 데이터를 문장별로 단어 토큰화하게 만드는 함수 생성\n",
    "def tokenize_text(text):\n",
    "    \n",
    "    # 문장별로 분리 토큰\n",
    "    sentences = sent_tokenize(text)\n",
    "    \n",
    "    # 분리된 문장별 단어 토큰화\n",
    "    word_tokens = [word_tokenize(sentence) for sentence in sentences]\n",
    "    return word_tokens\n",
    "\n",
    "# 여러 문장에 대해 문장별 단어 토큰화 수행\n",
    "word_tokens = tokenize_text(text_sample)\n",
    "print(type(word_tokens), len(word_tokens))\n",
    "print(word_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 스톱 워드 제거\n",
    "\n",
    "- 분석에 큰 의미가 없는 단어를 제거하는 것. ex) is, the, a, will -> 문맥적으로는 큰 의미가 없음.\n",
    "- 문법적인 특성으로 인해 특히 빈번하게 텍스트에 나타나지만 그 빈번함으로 인해 오히려 중요한 단어로 인지될 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\이혜림\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "영어 stop words 개수 179\n",
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download(\"stopwords\") # 언어별 스톱 워드를 제공\n",
    "\n",
    "print(\"영어 stop words 개수\", len(nltk.corpus.stopwords.words(\"english\")))\n",
    "print(nltk.corpus.stopwords.words(\"english\")[:20]) # 리스트 형식으로 스톱 워드가 담겨있는 리스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['matrix', 'everywhere', 'aroud', 'us', ',', 'even', 'room', '.'], ['see', 'window', 'television', '.'], ['feel', 'go', 'work', ',', 'go', 'church', 'pay', 'taxes', '.']]\n"
     ]
    }
   ],
   "source": [
    "stopwords = nltk.corpus.stopwords.words(\"english\")\n",
    "all_tokens = []\n",
    "\n",
    "# 위 예제에서 3개의 문장별로 얻은 word_tokens list에 대해 스톱 워드를 제거하는 반복문\n",
    "for sentence in word_tokens:\n",
    "    filtered_words = []\n",
    "    # 개별 문장별로 토큰화된 문장 list에 대해 스톱 워드를 제거하는 반복문\n",
    "    for word in sentence:\n",
    "        # 소문자로 모두 변환\n",
    "        word = word.lower()\n",
    "        # 토큰화된 개별 단어가 스톱 워드의 단어에 포함되지 않으면 word_tokens에 추가\n",
    "        if word not in stopwords:\n",
    "            filtered_words.append(word)\n",
    "    all_tokens.append(filtered_words)\n",
    "\n",
    "print(all_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming & Lemmatization\n",
    "\n",
    "- 문법적인 요소에 따라 단어가 다양하게 변화. 영어의 경우 과거/현재, 3인칭 단수 여부, 진행형 등 매우 많은 조건에 따라 원래 단어가 변화.\n",
    "- Lemmatization이 Stemming보다 정교하며 의미론적인 기반에서 단어의 원형을 찾음.따라서 변환에 더 오랜 시간을 필요로 함\n",
    "- Stemming : 일반적인 방법을 적용하거나 더 단순화된 방법을 적용하여 원래 단어에서 일부 철자가 훼손된 어근 단어를 추출\n",
    "    - nltk의 stemmer : 진행형, 3인칭 단수, 과거형에 따른 동사, 비교, 최상에 따른 형용사의 변화에 따라 찾아줌\n",
    "- Lemmatization : 품사와 같은 문법적인 요소와 더 의미적인 부분을 감안해 정확한 철자로 된 어근 단어를 찾아줌"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "work work work\n",
      "amus amus amus\n",
      "happy happiest\n",
      "fant fanciest\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import LancasterStemmer\n",
    "stemmer = LancasterStemmer()\n",
    "\n",
    "print(stemmer.stem(\"working\"), stemmer.stem(\"works\"), stemmer.stem(\"worked\"))\n",
    "print(stemmer.stem(\"amusing\"), stemmer.stem(\"amuses\"), stemmer.stem(\"amused\"))\n",
    "print(stemmer.stem(\"happier\"), stemmer.stem(\"happiest\"))\n",
    "print(stemmer.stem(\"fancier\"), stemmer.stem(\"fanciest\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\이혜림\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "amuse amuse amuse\n",
      "happy happy\n",
      "fancy fancy\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "nltk.download(\"wordnet\")\n",
    "\n",
    "lemma = WordNetLemmatizer()\n",
    "print(lemma.lemmatize(\"amusing\",\"v\"), lemma.lemmatize(\"amuses\", \"v\"), lemma.lemmatize(\"amused\",\"v\"))\n",
    "print(lemma.lemmatize(\"happier\",\"a\"), lemma.lemmatize(\"happiest\", \"a\"))\n",
    "print(lemma.lemmatize(\"fancier\",\"a\"), lemma.lemmatize(\"fanciest\",\"a\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
