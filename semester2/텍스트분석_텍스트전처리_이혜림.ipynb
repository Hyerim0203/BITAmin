{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 텍스트분석"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 머신러닝 알고리즘은 숫자형의 피처기반 데이터만 입력받을 수 있음\n",
    "- 픽터 벡터화 : 텍스트를 word 기반의 다수의 피처로 추출하고 이 피처에 단어 빈도수와 같은 숫자 값을 부여하여 단어의 조합인 벡터값으로 표현하는 것\n",
    "    - BOW(Bag of Words)\n",
    "    - Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 텍스트 분석 수행 프로세스\n",
    "\n",
    "1. 텍스트 사전 준비작업 : 클렌징, 대/소문자 변경, 특수문자 삭제 등의 클렌징 작업, 토큰화 작업, 의미 없는 단어 제거(Stop word) 작업, 어근 추출 등의 텍스트 정규화 작업\n",
    "1. 피터 벡터화/추출 : 가공된 텍스트에서 피처를 추출하고 여기에 벡터 값을 할당, BOW, Word2Vec\n",
    "1. ML 모델 수립 및 학습/예측/평가"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 텍스트 사전 준비작업"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 클렌징\n",
    "- 클렌징 : 텍스트에서 분석에 오히려 방해가 되는 불필요한 문자, 기호 등을 사전에 제거하는 작업. ex) HTML, XML 태그/특정 기호"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 텍스트 토큰화\n",
    "- 문서에서 문장을 분리하는 **문장 토큰화**와 문장에서 단어를 분리하는 **단어 토큰화**\n",
    "    - 문장 토큰화 : 마침표, 개행문자(\\n) 등 문장의 마지막을 뜻하는 기호에 따라 분리.\n",
    "    - 단어 토큰화 : 문장을 단어로 토큰화. 공백, 콤마, 마침표, 개행문자 등으로 단어를 분리. 단어의 순서가 중요하지 않은 경우 문장 토큰화 없이 단어 토큰화만 사용해도 충분.(단어 토큰화하면서 각 문장이 다 떼어지기 때문)\n",
    "        - 단어 토큰화를 하게 되면 문맥적인 의미는 무시될 수 밖에 없음. 이러한 문제를 해결하기 위해 도입된 것이 n-gram,\n",
    "        - n-gram은 연속된 n개의 단어를 하나의 토큰화 단위로 분리해 냄.\n",
    "        - ex) \"Agent smith knokds the door\" -> 2-gram\n",
    "            - (Agent, smith), (smith, knokds), (knokds, the), (the, door)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 문장 토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import sent_tokenize # 문장 토큰화 import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\이혜림\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download(\"punkt\") # 단어사전과 같이 참조가 필요한 데이터 세트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_sample = \"The Matrix is everywhere its all aroud us, here even in the room.\\\n",
    "    You can see it out your window or on your television.\\\n",
    "    You feel it when you go to work, or go to church or pay your taxes.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'> 3\n",
      "['The Matrix is everywhere its all aroud us, here even in the room.', 'You can see it out your window or on your television.', 'You feel it when you go to work, or go to church or pay your taxes.']\n"
     ]
    }
   ],
   "source": [
    "sentences = sent_tokenize(text=text_sample)\n",
    "print(type(sentences), len(sentences))\n",
    "print(sentences)\n",
    "# 각각의 문장으로 구성된 list 객체를 반환"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 단어 토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The',\n",
       " 'Matrix',\n",
       " 'is',\n",
       " 'everywhere',\n",
       " 'its',\n",
       " 'all',\n",
       " 'aroud',\n",
       " 'us',\n",
       " ',',\n",
       " 'here',\n",
       " 'even',\n",
       " 'in',\n",
       " 'the',\n",
       " 'room',\n",
       " '.']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import word_tokenize # 단어 토큰화 import\n",
    "word_tokenize(sentences[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 문장 토큰화 & 단어 토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'> 3\n",
      "[['The', 'Matrix', 'is', 'everywhere', 'its', 'all', 'aroud', 'us', ',', 'here', 'even', 'in', 'the', 'room', '.'], ['You', 'can', 'see', 'it', 'out', 'your', 'window', 'or', 'on', 'your', 'television', '.'], ['You', 'feel', 'it', 'when', 'you', 'go', 'to', 'work', ',', 'or', 'go', 'to', 'church', 'or', 'pay', 'your', 'taxes', '.']]\n"
     ]
    }
   ],
   "source": [
    "# 여러 개의 문장으로 된 입력 데이터를 문장별로 단어 토큰화하게 만드는 함수 생성\n",
    "def tokenize_text(text):\n",
    "    \n",
    "    # 문장별로 분리 토큰\n",
    "    sentences = sent_tokenize(text)\n",
    "    \n",
    "    # 분리된 문장별 단어 토큰화\n",
    "    word_tokens = [word_tokenize(sentence) for sentence in sentences]\n",
    "    return word_tokens\n",
    "\n",
    "# 여러 문장에 대해 문장별 단어 토큰화 수행\n",
    "word_tokens = tokenize_text(text_sample)\n",
    "print(type(word_tokens), len(word_tokens))\n",
    "print(word_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *토큰화에서 고려해야할 사항*\n",
    "\n",
    "1. 구두점이나 특수문자를 단순 제외해서는 안된다.\n",
    "    - 예를들어 온점과 같은 경우는 문장의 경계를 알 수 있는데 도움이 되기 때문에 온점을 제외하지 않을 수 있음\n",
    "    - 단어 자체에서 구두점을 가지고 있는 경우(Ph.D, $500)\n",
    "    - 숫자 사이에 컴마가 들어가 있는 경우\n",
    "    - 온점이 주로 어떤 약어로 쓰이는지를 정리하여 nltk, openNLP 등은 이를 이용하여 문장의 마침표에 쓰이는 온점을 구분하고 있음\n",
    "1. 줄임말과 단어 내에 띄어쓰기가 있는 경우\n",
    "    - what're\n",
    "    - 단어 내에 띄어쓰기가 있는 경우(rock 'n' roll)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *한국어 토큰화*\n",
    "\n",
    "1. 한국어는 교착어이다.\n",
    "    - 영어가 띄어쓰기를 기준으로 구분하기 용이한 것에 비해, 한국어는 단어 바로 다음에 의존형태소가 붙기 때문에(ex 그는, 그가) 형태소를 기준으로 구분해주어야 함\n",
    "    \n",
    "1. 한국어는 띄어쓰기가 영어보다 잘 지켜지지 않음\n",
    "1. 영어와 마찬가지로 품사에 따라서 의미가 달라짐"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 스톱 워드 제거\n",
    "\n",
    "- 분석에 큰 의미가 없는 단어를 제거하는 것. ex) is, the, a, will -> 문맥적으로는 큰 의미가 없음.\n",
    "    - 영어권에서는 대부분 단어의 길이가 길기 때문에 2~3길이의 단어를 제거하는 것만으로도 큰 효과(한국어는 한자어기 때문에 다름)\n",
    "    - 등장 빈도가 너무 적은 단어\n",
    "- 대,소문자 통합\n",
    "    - 대,소문자를 통합함(문장의 맨 첫번째 알파벳은 대문자)으로써 단어를 많이 통합할 수 있지만, us/US, 회사이름과 같이 대소문자에 따라 그 의미가 달라지는 것에는 부적합\n",
    "        - 단어 맨앞의 알파벳은 소문자로\n",
    "        - 나머지는 그대로\n",
    "    - 하지만 만약 대소문자를 정확히 구분하지 않고 쓴 문장에 대해서는 이를 모두 소문자로 통합하는 것이 더 좋을 수 있음\n",
    "- 문법적인 특성으로 인해 특히 빈번하게 텍스트에 나타나지만 그 빈번함으로 인해 오히려 중요한 단어로 인지될 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\이혜림\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "영어 stop words 개수 179\n",
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download(\"stopwords\") # 언어별 스톱 워드를 제공\n",
    "\n",
    "print(\"영어 stop words 개수\", len(nltk.corpus.stopwords.words(\"english\")))\n",
    "print(nltk.corpus.stopwords.words(\"english\")[:20]) # 리스트 형식으로 스톱 워드가 담겨있는 리스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['matrix', 'everywhere', 'aroud', 'us', ',', 'even', 'room', '.'], ['see', 'window', 'television', '.'], ['feel', 'go', 'work', ',', 'go', 'church', 'pay', 'taxes', '.']]\n"
     ]
    }
   ],
   "source": [
    "stopwords = nltk.corpus.stopwords.words(\"english\")\n",
    "all_tokens = []\n",
    "\n",
    "# 위 예제에서 3개의 문장별로 얻은 word_tokens list에 대해 스톱 워드를 제거하는 반복문\n",
    "for sentence in word_tokens:\n",
    "    filtered_words = []\n",
    "    # 개별 문장별로 토큰화된 문장 list에 대해 스톱 워드를 제거하는 반복문\n",
    "    for word in sentence:\n",
    "        # 소문자로 모두 변환\n",
    "        word = word.lower()\n",
    "        # 토큰화된 개별 단어가 스톱 워드의 단어에 포함되지 않으면 word_tokens에 추가\n",
    "        if word not in stopwords:\n",
    "            filtered_words.append(word)\n",
    "    all_tokens.append(filtered_words)\n",
    "\n",
    "print(all_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming & Lemmatization\n",
    "\n",
    "- 문법적인 요소에 따라 단어가 다양하게 변화. 영어의 경우 과거/현재, 3인칭 단수 여부, 진행형 등 매우 많은 조건에 따라 원래 단어가 변화.\n",
    "- Lemmatization이 Stemming보다 정교하며 의미론적인 기반에서 단어의 원형을 찾음.따라서 변환에 더 오랜 시간을 필요로 함\n",
    "- Stemming : 일반적인 방법을 적용하거나 더 단순화된 방법을 적용하여 원래 단어에서 일부 철자가 훼손된 어근 단어를 추출\n",
    "    - nltk의 stemmer : 진행형, 3인칭 단수, 과거형에 따른 동사, 비교, 최상에 따른 형용사의 변화에 따라 찾아줌\n",
    "- Lemmatization : 품사와 같은 문법적인 요소와 더 의미적인 부분을 감안해 정확한 철자로 된 어근 단어를 찾아줌"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "work work work\n",
      "amus amus amus\n",
      "happy happiest\n",
      "fant fanciest\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import LancasterStemmer\n",
    "stemmer = LancasterStemmer()\n",
    "\n",
    "print(stemmer.stem(\"working\"), stemmer.stem(\"works\"), stemmer.stem(\"worked\"))\n",
    "print(stemmer.stem(\"amusing\"), stemmer.stem(\"amuses\"), stemmer.stem(\"amused\"))\n",
    "print(stemmer.stem(\"happier\"), stemmer.stem(\"happiest\"))\n",
    "print(stemmer.stem(\"fancier\"), stemmer.stem(\"fanciest\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\이혜림\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "amuse amuse amuse\n",
      "happy happy\n",
      "fancy fancy\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "nltk.download(\"wordnet\")\n",
    "\n",
    "lemma = WordNetLemmatizer()\n",
    "print(lemma.lemmatize(\"amusing\",\"v\"), lemma.lemmatize(\"amuses\", \"v\"), lemma.lemmatize(\"amused\",\"v\"))\n",
    "print(lemma.lemmatize(\"happier\",\"a\"), lemma.lemmatize(\"happiest\", \"a\"))\n",
    "print(lemma.lemmatize(\"fancier\",\"a\"), lemma.lemmatize(\"fanciest\",\"a\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
